{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>एक्सावेटा की एक विशेषता क्या है?\\n</td>\n",
       "      <td>संघ के रूप में इलाज किया जा सकता है जो प्रोटिस...</td>\n",
       "      <td>मोनोफाईलेटिक</td>\n",
       "      <td>209</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>किस अम्ल में सबसे अधिक पशु चर्बी प्रतिशत होता है?</td>\n",
       "      <td>साँचा:Chembox ThermalConductivityसाँचा:Chembox...</td>\n",
       "      <td>स्टीयरिक अम्ल</td>\n",
       "      <td>132</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>चीन में 6वी जनसंख्या गणना कब आयोजित की गई?</td>\n",
       "      <td>चीन की जनसांख्यिकीय अपेक्षाकृत छोटे युवा घटक क...</td>\n",
       "      <td>1 नवंबर 2010</td>\n",
       "      <td>441</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>चीन की जनसंख्या वृद्धि दर कितनी है?</td>\n",
       "      <td>चीन की जनसांख्यिकीय अपेक्षाकृत छोटे युवा घटक क...</td>\n",
       "      <td>0.59%</td>\n",
       "      <td>389</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ब्रिटनी क्या चाहती है?</td>\n",
       "      <td>अपने रिकॉर्डिंग की जीवन-वृत्ति के शुभारंभ के त...</td>\n",
       "      <td>दीवार की पोस्टर में दीखना</td>\n",
       "      <td>1381</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6019</th>\n",
       "      <td>संधात्मक गणतंत्र किस तरह की व्यवस्था है?\\n</td>\n",
       "      <td>राष्ट्रपति प्रणाली के साथ सरकार का रूप लोकतांत...</td>\n",
       "      <td>राष्ट्रपति प्रणाली</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020</th>\n",
       "      <td>किस देश में विधायिका को विधि का स्रोत कहा जाता...</td>\n",
       "      <td>राष्ट्रपति प्रणाली के साथ सरकार का रूप लोकतांत...</td>\n",
       "      <td>ब्राजील</td>\n",
       "      <td>435</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>उड़ने वाली वस्तु एक प्रमुख निर्यात क्या था?</td>\n",
       "      <td>आईएमएफ डेटा के अनुसार  2017 में ब्राजील को 77व...</td>\n",
       "      <td>विमान</td>\n",
       "      <td>910</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6022</th>\n",
       "      <td>विभाजन के बाद शेष कौन से शासित प्रदेश केंद्र श...</td>\n",
       "      <td>भारत ने 1947 में अंग्रेजों से स्वतंत्रता प्राप...</td>\n",
       "      <td>दमन और दीव</td>\n",
       "      <td>377</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6023</th>\n",
       "      <td>मुख्यालय किस स्थान पर है?\\n</td>\n",
       "      <td>जामनगर ज़िला भारत के गुजरात राज्य का एक ज़िला ...</td>\n",
       "      <td>जामनगर</td>\n",
       "      <td>68</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6024 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                    एक्सावेटा की एक विशेषता क्या है?\\n   \n",
       "1     किस अम्ल में सबसे अधिक पशु चर्बी प्रतिशत होता है?   \n",
       "2            चीन में 6वी जनसंख्या गणना कब आयोजित की गई?   \n",
       "3                   चीन की जनसंख्या वृद्धि दर कितनी है?   \n",
       "4                                ब्रिटनी क्या चाहती है?   \n",
       "...                                                 ...   \n",
       "6019         संधात्मक गणतंत्र किस तरह की व्यवस्था है?\\n   \n",
       "6020  किस देश में विधायिका को विधि का स्रोत कहा जाता...   \n",
       "6021        उड़ने वाली वस्तु एक प्रमुख निर्यात क्या था?   \n",
       "6022  विभाजन के बाद शेष कौन से शासित प्रदेश केंद्र श...   \n",
       "6023                        मुख्यालय किस स्थान पर है?\\n   \n",
       "\n",
       "                                                context  \\\n",
       "0     संघ के रूप में इलाज किया जा सकता है जो प्रोटिस...   \n",
       "1     साँचा:Chembox ThermalConductivityसाँचा:Chembox...   \n",
       "2     चीन की जनसांख्यिकीय अपेक्षाकृत छोटे युवा घटक क...   \n",
       "3     चीन की जनसांख्यिकीय अपेक्षाकृत छोटे युवा घटक क...   \n",
       "4     अपने रिकॉर्डिंग की जीवन-वृत्ति के शुभारंभ के त...   \n",
       "...                                                 ...   \n",
       "6019  राष्ट्रपति प्रणाली के साथ सरकार का रूप लोकतांत...   \n",
       "6020  राष्ट्रपति प्रणाली के साथ सरकार का रूप लोकतांत...   \n",
       "6021  आईएमएफ डेटा के अनुसार  2017 में ब्राजील को 77व...   \n",
       "6022  भारत ने 1947 में अंग्रेजों से स्वतंत्रता प्राप...   \n",
       "6023  जामनगर ज़िला भारत के गुजरात राज्य का एक ज़िला ...   \n",
       "\n",
       "                         answer  answer_start  answer_end  \n",
       "0                  मोनोफाईलेटिक           209         221  \n",
       "1                 स्टीयरिक अम्ल           132         145  \n",
       "2                  1 नवंबर 2010           441         453  \n",
       "3                         0.59%           389         394  \n",
       "4     दीवार की पोस्टर में दीखना          1381        1406  \n",
       "...                         ...           ...         ...  \n",
       "6019         राष्ट्रपति प्रणाली             0          18  \n",
       "6020                    ब्राजील           435         442  \n",
       "6021                      विमान           910         915  \n",
       "6022                 दमन और दीव           377         387  \n",
       "6023                     जामनगर            68          74  \n",
       "\n",
       "[6024 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForQuestionAnswering, AutoModelForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Preprocess the IndicQA Dataset\n",
    "def preprocess_indicqa_for_extractive(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    data = []\n",
    "    for entry in raw_data[\"data\"]:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa[\"answers\"]\n",
    "                if answers and answers[0][\"text\"]:  # If there's a valid answer\n",
    "                    answer = answers[0][\"text\"]\n",
    "                    answer_start = answers[0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer)\n",
    "                    data.append({\"question\": question, \"context\": context, \"answer\": answer, \n",
    "                                 \"answer_start\": answer_start, \"answer_end\": answer_end})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data_file = \"mergedQuAD-hi-train.json\"  # Replace with your actual JSON file path\n",
    "df = preprocess_indicqa_for_extractive(data_file)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /data2/home/dileeppatel/miniconda3\n",
      "DataAnalytics            /data2/home/dileeppatel/miniconda3/envs/DataAnalytics\n",
      "dlnlp                 *  /data2/home/dileeppatel/miniconda3/envs/dlnlp\n",
      "trans_env                /data2/home/dileeppatel/miniconda3/envs/trans_env\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Step 2: Split into Train, Validation, and Test Sets\n",
    "train_df = df.sample(frac=0.8, random_state=42).reset_index(drop=True)\n",
    "remaining_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "valid_df = remaining_df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "test_df = remaining_df.drop(valid_df.index).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class QAExtractiveDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        question = row[\"question\"]\n",
    "        context = row[\"context\"]\n",
    "        answer = row[\"answer\"]\n",
    "        answer_start = row[\"answer_start\"]\n",
    "        answer_end = row[\"answer_end\"]\n",
    "\n",
    "        # Tokenize question and context together\n",
    "        inputs = self.tokenizer(\n",
    "            question, context, \n",
    "            max_length=self.max_len, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Find start and end token positions\n",
    "        offset_mapping = inputs[\"offset_mapping\"].squeeze()\n",
    "        start_token_idx = end_token_idx = 0\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start <= answer_start < end:\n",
    "                start_token_idx = i\n",
    "            if start < answer_end <= end:\n",
    "                end_token_idx = i\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"start_positions\": torch.tensor(start_token_idx),\n",
    "            \"end_positions\": torch.tensor(end_token_idx),\n",
    "            \"answer\": answer,  # Include answer for metrics\n",
    "        }\n",
    "\n",
    "\n",
    "# model_name = \"xlm-roberta-base\"  \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = XLMRobertaForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# model_name = \"google/muril-base-cased\"  \n",
    "\n",
    "# model_name = \"ai4bharat/IndicBERTv2-MLM-Sam-TLM\"\n",
    "\n",
    "#model_name = \"ai4bharat/indicbart\"\n",
    "\n",
    "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_dataset = QAExtractiveDataset(train_df, tokenizer)\n",
    "valid_dataset = QAExtractiveDataset(valid_df, tokenizer)\n",
    "test_dataset = QAExtractiveDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 151/151 [02:27<00:00,  1.02it/s]\n",
      "Validation: 100%|███████████████████████████████| 19/19 [00:09<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3152 | Validation Loss: 2.0066\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██████▌                         | 31/151 [00:30<01:57,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Define Training and Validation Loops\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                            start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    " \n",
    "# Step 7: Train and Evaluate the Model\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    valid_loss = validate_epoch(model, valid_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 30 14:53:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              59W / 400W |  12610MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          Off | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              58W / 400W |  40320MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          Off | 00000000:45:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              56W / 400W |  12288MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              84W / 400W |  36724MiB / 40960MiB |     36%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          Off | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              59W / 400W |  13718MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          Off | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              61W / 400W |  37222MiB / 40960MiB |     25%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          Off | 00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   47C    P0             294W / 400W |  36780MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          Off | 00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   62C    P0             317W / 400W |  37115MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     4298MiB |\n",
      "|    0   N/A  N/A   2559000      C   ...nsg/anaconda3/envs/dlnlp/bin/python     4256MiB |\n",
      "|    0   N/A  N/A   2886111      C   /bin/python3                               4030MiB |\n",
      "|    1   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     3796MiB |\n",
      "|    1   N/A  N/A   2835776      C   ...el/miniconda3/envs/dlnlp/bin/python    31814MiB |\n",
      "|    1   N/A  N/A   2899420      C   ...el/miniconda3/envs/dlnlp/bin/python     4684MiB |\n",
      "|    2   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     3792MiB |\n",
      "|    2   N/A  N/A   2650505      C   python                                     8478MiB |\n",
      "|    3   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     3792MiB |\n",
      "|    3   N/A  N/A   2879224      C   python3                                   32912MiB |\n",
      "|    4   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     3792MiB |\n",
      "|    4   N/A  N/A   2650505      C   python                                      436MiB |\n",
      "|    4   N/A  N/A   2872685      C   /usr/bin/python3                           9464MiB |\n",
      "|    5   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     3652MiB |\n",
      "|    5   N/A  N/A   2871263      C   python                                     1096MiB |\n",
      "|    5   N/A  N/A   2879224      C   python3                                   32448MiB |\n",
      "|    6   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     2764MiB |\n",
      "|    6   N/A  N/A   2801385      C   python                                    12430MiB |\n",
      "|    6   N/A  N/A   2893841      C   python3                                   21562MiB |\n",
      "|    7   N/A  N/A   2550329      C   ...nsg/anaconda3/envs/dlnlp/bin/python     2620MiB |\n",
      "|    7   N/A  N/A   2601453      C   ...nsg/anaconda3/envs/dlnlp/bin/python     5230MiB |\n",
      "|    7   N/A  N/A   2802442      C   python                                     4422MiB |\n",
      "|    7   N/A  N/A   2893841      C   python3                                   24814MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('indicbert-v2/tokenizer_config.json',\n",
       " 'indicbert-v2/special_tokens_map.json',\n",
       " 'indicbert-v2/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"indicbert-v2\")\n",
    "tokenizer.save_pretrained(\"indicbert-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForQuestionAnswering, AutoModelForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta\")\n",
    "model = XLMRobertaForQuestionAnswering.from_pretrained(\"roberta\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(model, tokenizer, dataloader):\n",
    "   \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "            start_indices = torch.argmax(start_logits, dim=1)\n",
    "            end_indices = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                pred_tokens = input_ids[i][start_indices[i]:end_indices[i] + 1]\n",
    "                predicted_answer = tokenizer.decode(pred_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                predictions.append(predicted_answer)\n",
    "                references.append(batch[\"answer\"][i])  # Ground truth answers\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "\n",
    "def evaluate_with_jaccard(predictions, references):\n",
    "   \n",
    "    # Calculate Exact Match\n",
    "    em = sum([1 if pred.strip() == ref.strip() else 0 for pred, ref in zip(predictions, references)]) / len(references)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    def compute_f1(pred, ref):\n",
    "        pred_tokens = set(pred.split())\n",
    "        ref_tokens = set(ref.split())\n",
    "        common_tokens = pred_tokens & ref_tokens\n",
    "        if not common_tokens:\n",
    "            return 0\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    f1 = sum([compute_f1(pred, ref) for pred, ref in zip(predictions, references)]) / len(references)\n",
    "\n",
    "    # Calculate Jaccard Score\n",
    "    def compute_jaccard(pred, ref):\n",
    "        pred_tokens = set(pred.split())\n",
    "        ref_tokens = set(ref.split())\n",
    "        intersection = len(pred_tokens & ref_tokens)\n",
    "        union = len(pred_tokens | ref_tokens)\n",
    "        if union == 0:\n",
    "            return 0\n",
    "        return intersection / union\n",
    "\n",
    "    jaccard = sum([compute_jaccard(pred, ref) for pred, ref in zip(predictions, references)]) / len(references)\n",
    "\n",
    "    return {\"Exact Match\": em, \"F1 Score\": f1, \"Jaccard Score\": jaccard}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 38/38 [00:09<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact Match: 78.28%\n",
      "F1 Score: 0.8541\n",
      "Jaccard Score: 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 19/19 [00:09<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact Match: 37.39%\n",
      "F1 Score: 0.5528\n",
      "Jaccard Score: 0.5093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 33/33 [00:17<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact Match: 47.34%\n",
      "F1 Score: 0.5947\n",
      "Jaccard Score: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 24/24 [00:19<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact Match: 34.05%\n",
      "F1 Score: 0.4702\n",
      "Jaccard Score: 0.4368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_test1 = preprocess_indicqa_for_extractive(\"mergedQuAD-hi-test.json\")\n",
    "df_test2 = preprocess_indicqa_for_extractive(\"indicqa.hi.json\")\n",
    "df_test3 = pd.read_csv('chaii-hindi.csv')\n",
    "\n",
    "\n",
    "test_dataset1 = QAExtractiveDataset(df_test1, tokenizer)\n",
    "test_dataset2 = QAExtractiveDataset(df_test2, tokenizer)\n",
    "test_dataset3 = QAExtractiveDataset(df_test3, tokenizer)\n",
    "#test_dataset = QAExtractiveDataset(test_df, tokenizer)\n",
    "\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=32)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=32)\n",
    "test_loader3 = DataLoader(test_dataset3, batch_size=32)\n",
    "\n",
    "\n",
    "for test_l in [test_loader, test_loader1, test_loader2, test_loader3]:\n",
    "    predictions, references = batch_inference(model, tokenizer, test_l)\n",
    "    metrics = evaluate_with_jaccard(predictions, references)\n",
    "    print(f\"\\nExact Match: {metrics['Exact Match'] * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Jaccard Score: {metrics['Jaccard Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, question, context, max_len=512):\n",
    "    \"\"\"\n",
    "    Perform inference on a single question-context pair.\n",
    "\n",
    "    Args:\n",
    "        model: Fine-tuned model.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        question (str): The question string.\n",
    "        context (str): The context string.\n",
    "        max_len (int): Maximum length for tokenized inputs.\n",
    "\n",
    "    Returns:\n",
    "        predicted_answer (str): The extracted answer.\n",
    "    \"\"\"\n",
    "    # Tokenize the inputs\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        context, \n",
    "        max_length=max_len, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "    # Get start and end token positions\n",
    "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "    # Decode the predicted answer tokens\n",
    "    predicted_tokens = input_ids[0][start_idx:end_idx + 1]\n",
    "    predicted_answer = tokenizer.decode(predicted_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: कलाम की मृत्यु पर शोक व्यक्त करते हुए किस सरकार ने श्रद्धांजलि में 1000 मक्खन के दीपक भेंट किए थे?\n",
      "Context: \"भूटान सरकार ने कलाम की मौत के शोक के लिए देश के झंडे को आधी ऊंचाई पर फहराने के लिए आदेश दिया, और श्रद्धांजलि में 1000 मक्खन के दीपक की भेंट किए।\"\n",
      "Predicted Answer: भूटान सरकार\n"
     ]
    }
   ],
   "source": [
    "question = \"कलाम की मृत्यु पर शोक व्यक्त करते हुए किस सरकार ने श्रद्धांजलि में 1000 मक्खन के दीपक भेंट किए थे?\"\n",
    "context = \"\\\"भूटान सरकार ने कलाम की मौत के शोक के लिए देश के झंडे को आधी ऊंचाई पर फहराने के लिए आदेश दिया, और श्रद्धांजलि में 1000 मक्खन के दीपक की भेंट किए।\\\"\"\n",
    "\n",
    "# Predict the answer\n",
    "predicted_answer = run_inference(model, tokenizer, question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
